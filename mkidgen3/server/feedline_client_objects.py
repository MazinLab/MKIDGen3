import copy
import pickle
import threading
import time
import numpy as np
import zmq
import blosc2
from typing import List
from npy_append_array import NpyAppendArray
from logging import getLogger
from datetime import datetime
import os
from hashlib import md5
from mkidgen3.server.misc import zpipe
from mkidgen3.funcs import compute_lo_steps, convert_adc_raw_to_mv, raw_iq_to_unit, raw_phase_to_radian
from ..system_parameters import SYSTEM_BANDWIDTH
from mkidgen3.rfsocmemory import memfree_mib
from .feedline_config import FeedlineConfig, WaveformConfig, IFConfig
from ..mkidpynq import PHOTON_DTYPE
from ..system_parameters import N_CHANNELS, N_IQ_GROUPS, N_PHASE_GROUPS, N_POSTAGE_CHANNELS
from functools import cached_property

from typing import Tuple


class CaptureAbortedException(Exception):
    pass


class FRSClient:
    def __init__(self, url: str, command_port: int = 10000, data_port: int = 10001, status_port: int = 10002):
        """
        Args:
            url: ip address of rfsoc for ex: rfsoc4x2b.physics.ucsb.edu
            command_port: port for feedline server commands
            data_port: port for capture data server
            status_port: port for status server
        """
        self.url = url
        self.command_port = command_port
        self.data_port = data_port
        self.status_port = status_port

    @property
    def command_url(self):
        return f'tcp://{self.url}:{self.command_port}'

    @property
    def data_url(self):
        return f'tcp://{self.url}:{self.data_port}'

    @property
    def status_url(self):
        return f'tcp://{self.url}:{self.status_port}'

    def __str__(self):
        return f'FRS@{self.url}:{self.command_port}'

    def bequiet(self):
        ctx = zmq.Context().instance()
        with ctx.socket(zmq.REQ) as s:
            s.connect(self.command_url)
            s.send_pyobj(('bequiet', {}))
            return s.recv_pyobj()


class CaptureRequest:
    """
    A Capture Request object is generated by the client and submitted to the feedline readout server
    """
    STATUS_ENDPOINT = 'inproc://cap_stat.xsub'
    DATA_ENDPOINT = 'inproc://cap_data.xsub'
    FINAL_STATUSES = ('finished', 'aborted', 'failed')
    SUPPORTED_TAPS = ('postage', 'photon', 'adc', 'iq', 'phase')

    @staticmethod
    def validate_channels(tap: str, chan: list):
        """
        Verify that the channel/group selection is compatible with the driver.

        Note that this is not done dynamically, no driver discovery is performed

        See mkidgen3.drivers.FilterPhase, .drivers.FilterIQ, and .trigger.PhotonPostageFilter
        for details about groups and monitor channels.

        Args:
            chan: A list of channels/groups to monotor
            tap: A capture location

        Returns: True|False
        """
        tap = tap.lower()
        if tap == 'iq':
            l = N_IQ_GROUPS
            m = N_IQ_GROUPS
        elif tap == 'phase':
            l = N_PHASE_GROUPS
            m = N_PHASE_GROUPS
        elif tap == 'postage':
            l = N_POSTAGE_CHANNELS
            m = N_CHANNELS
        elif chan:
            l = 0  # no channels for photon or adc capture
        else:
            return False
        try:
            assert len(chan) <= l
            for c in chan:
                assert type(c) == int and 0 <= c < m
        except AssertionError:
            return False
        return True

    def __init__(self, n, tap: str, feedline_config: FeedlineConfig,
                 feedline_server: FRSClient, channels: list = None, file: str = None):
        """

        Args:
            n: the number of samples to capture, for photons n is the buffer rotation interval request in ms
            tap: the location to capture: adc|iq|phase|postage|photon
            feedline_config: the FL configuration required by the capture
            feedline_server: the FRS to capture from
            file: an optinal file
            channels: an optional (required for postage) specifier of which channels to monitor.
        """
        tap = tap.lower()
        assert tap in CaptureRequest.SUPPORTED_TAPS
        self.nsamp = n  # n is treated as the buffer time in ms for photons, and has limits enforced by the driver
        self._last_status = None
        if channels is not None:
            if not CaptureRequest.validate_channels(tap, channels):
                raise ValueError('Invalid channels specification')
        self.channels = list(channels) if channels else None
        self.tap = tap  # maybe add some error handling here
        self.feedline_config = feedline_config
        self.server = feedline_server
        self._status_socket = None
        self._data_socket = None
        self._established = False
        self.data_endpoint = file or type(self).DATA_ENDPOINT

    def __hash__(self):
        return int(md5(str((hash(self.feedline_config), self.tap, self.data_endpoint,
                            self.nsamp, self.server.command_url)).encode()).hexdigest(), 16)

    # def __enter__(self):
    #     self.establish()
    #
    # def __exit__(self, exc_type, exc_val, exc_tb):
    #     if isinstance(exc_type, zmq.error.ContextTerminated):
    #         self._data_socket=None
    #         self._status_socket=None
    #     else:
    #         self.destablish()

    def __del__(self):
        try:
            self.destablish()
        except zmq.error.ContextTerminated:
            pass

    def __str__(self):
        return f'CapReq {self.server}:{self.tap} {str(hash(self))}'

    def submit(self, ctx: zmq.Context = None):
        ctx = ctx or zmq.Context().instance()
        with ctx.socket(zmq.REQ) as s:
            s.connect(self.server.command_url)
            s.send_pyobj(('capture', self))
            return s.recv_pyobj()

    @property
    def type(self):
        return 'engineering' if self.tap in ('adc', 'iq', 'phase') else self.tap

    @property
    def id(self):
        return str(hash(self)).encode()

    def establish(self, context: zmq.Context = None):
        """
        Open up the outbound sockets for status and data. If data is going to a file, verify it ca be opened for writing and close it
        """
        if self._status_socket is not None or self._data_socket is not None:
            getLogger(__name__).warning(f'{self} already established')
            return
        context = context or zmq.Context.instance()
        self._status_socket = context.socket(zmq.PUB)
        self._status_socket.connect(self.STATUS_ENDPOINT)
        if self.data_endpoint.startswith('file://'):
            f = self.data_endpoint.lstrip('file://')
            if not os.access('/' + os.path.dirname(f), os.W_OK):
                raise IOError(f'Unable to write to {f}')
            return
        self._data_socket = context.socket(zmq.PUB)
        self._data_socket.connect(self.data_endpoint)
        self._send_status('established')

    def destablish(self):
        try:
            # NB the context or socket's linger setting determines how long any pending messages have to get sent off
            self._status_socket.close()
            self._status_socket = None
        except AttributeError:
            pass
        try:
            self._data_socket.close()
            self._data_socket = None
        except AttributeError:
            pass

    def fail(self, message, raise_exception=False):
        if self.completed:
            return
        try:
            if self.data_endpoint.startswith('file://'):
                pass
            else:
                self._data_socket.send_multipart([self.id, b''])
            self._send_status('failed', message)
            self.destablish()
        except zmq.ZMQError as ez:
            getLogger(__name__).warning(f'Failed to send fail message {self} due to {ez}')
            if raise_exception:
                raise ez

    def finish(self, raise_exception=True):
        if self.completed:
            return
        try:
            if self.data_endpoint.startswith('file://'):
                pass
            else:
                self._data_socket.send_multipart([self.id, b''])
            self._send_status('finished', f'Finished at UTC {datetime.utcnow()}')
            self.destablish()
        except zmq.ZMQError as ez:
            getLogger(__name__).warning(f'Failed to send finished message {self} due to {ez}')
            if raise_exception:
                raise ez

    def abort(self, message, raise_exception=False):
        if self.completed:
            return
        try:
            self._data_socket.send_multipart([self.id, b''])
            self._send_status('aborted', message)
            self.destablish()
        except zmq.ZMQError as ez:
            getLogger(__name__).warning(f'Failed to send abort message {self} due to {ez}')
            if raise_exception:
                raise ez

    def send_data(self, data, status='', copy=False, compress=True):
        """
        Send a (chunk) of data out. By default data is published as a blosc2 compressed array with the
        capture id as the subscription key. CaptureRequests created with a file destination  will be saved as
        uncompressed npy data, appended on successive calls, no data will be published.

        Data is shipped in the (compressed) format it is given.

        Do not call this function from multiple threads.
        Call .establish() in the calling thread before calling this function or after calling destablish()

        This function is intended to be zero-copy, is as of 10/27/23 and should be maintained as such.

        Args:
            data: a array of data to send out
            status: (optional) a status update message to send out
            copy: copy the array first, you probably don't want this. copying the array then freeing the
            underlying buffer causes segfaults with the pycharm debugger. Be aware that the property
            zmq.COPY_THRESHOLD will case messages to be copied even if copy is False if the message size
            is less than the threshold.
            compress: compress the data with blosc2, ignored for file destinations.

        Returns: None | zmq.MessageTracker None is returned if copying or the destination is a file. A
        segfault may occur if the data is deallocated prior to the messagetracker being done.
        """
        if not self._status_socket:
            raise RuntimeError('Status socket is not established.')
        if self.data_endpoint.startswith('file://'):
            with NpyAppendArray('/' + self.data_endpoint.lstrip('file://'), delete_if_exists=False) as x:
                x.append(data)
            self._send_status('capturing', status)
            return
        elif not self._data_socket:
            raise RuntimeError('Data socket is not established.')

        #        getLogger(__name__).debug(f'MiB Free: {memfree_mib()}')
        data = np.array(data.tolist()) if copy else data
        times = []
        times.append(time.time())
        #        getLogger(__name__).debug(f'MiB Free: {memfree_mib()}')
        compressed = blosc2.compress(data) if compress else data
        times.append(time.time())
        #        getLogger(__name__).debug(f'MiB Free: {memfree_mib()}')
        lend = len(compressed) / 1024 ** 2
        times.append(time.time())
        #        getLogger(__name__).debug(f'MiB Free: {memfree_mib()}')
        self._send_status('capturing', status)
        times.append(time.time())
        tracker = self._data_socket.send_multipart([self.id, compressed], copy=False, track=not copy)
        times.append(time.time())
        getLogger(__name__).debug(list(zip(('Compress', 'Len compute', 'Status', 'Ship'),
                                           (np.diff(times) * 1000).astype(int))))
        getLogger(__name__).debug(
            f'Sending {lend:.1f} MiB, compressed to {100 * lend / (data.nbytes / 1024 ** 2):.1f}%')
        return tracker

    def _send_status(self, status, message=''):
        if not self._status_socket:
            raise RuntimeError('No status_socket connection available')
        update = f'{status}:{message}'
        getLogger(__name__).debug(f'Published status update for {self}: "{update}"')
        self._last_status = (status, message)
        self._status_socket.send_multipart([self.id, update.encode()])

    @property
    def completed(self):
        return self._last_status is not None and self._last_status[0] in ('finished', 'aborted', 'failed')

    def set_status(self, status, message='', context: zmq.Context = None, destablish=False):
        """
        get appropriate context and send current status message after connecting the status socket

        context is ignored if a status destination connection is extant
        """
        if not self._status_socket:
            context = context or zmq.Context().instance()
            self._status_socket = context.socket(zmq.PUB)
            self._status_socket.connect(self.STATUS_ENDPOINT)
        self._send_status(status, message)
        if destablish:
            self.destablish()

    @property
    def size_bytes(self):
        return self.nsamp * self.nchan * self.dwid

    @property
    def nchan(self):
        return 2048 if self.tap in ('iq', 'phase') else 1

    @property
    def ngroup(self):
        return 256 if self.tap in ('iq', 'phase') else 1

    @property
    def dwid(self):
        """Data size of sample in bytes"""
        return 4 if self.tap in ('adc', 'iq') else 2

    @property
    def buffer_shape(self):
        return self.nsamp, self.nchan, self.dwid // 2


class CaptureSink(threading.Thread):
    def __init__(self, req_nfo: [CaptureRequest, tuple], start=True):
        if isinstance(req_nfo, CaptureRequest):
            id, buffer_shape, source_url = req_nfo.id, req_nfo.buffer_shape, req_nfo.server.data_url
        else:
            id, buffer_shape, source_url = req_nfo
        #        if isinstance(id, bytes):
        #            id = id.decode()
        super(CaptureSink, self).__init__(name=f'cap_id={id}')
        self._expected_buffer_shape = buffer_shape
        self.daemon = True
        self.cap_id = id
        self.data_source = source_url
        self.result = None
        self._pipe = zpipe(zmq.Context.instance())
        self._buf = []
        self.socket = None
        if start:
            self.start()

    def kill(self):
        self._pipe[0].send(b'')
        self._pipe[0].close()

    def _accumulate_data(self, d):
        self._buf.append(d)

    def _finish_accumulation(self):
        self._buf = b''.join(self._buf)

    def _finalize_data(self):
        self.result = np.frombuffer(self._buf, dtype=np.int16)

    def establish(self, ctx: zmq.Context = None):
        ctx = ctx or zmq.Context.instance()
        self.socket = ctx.socket(zmq.SUB)
        self.socket.setsockopt(zmq.SUBSCRIBE, self.cap_id)
        self.socket.connect(self.data_source)
        return self.socket

    def receive(self):
        id, data = self.socket.recv_multipart(copy=False)
        if not data:
            self.socket.close()
            return None
        return blosc2.decompress(data)

    def destablish(self):
        self.socket.close()
        self.socket = None

    def run(self):
        assert self.socket is None
        try:
            self.establish()
            poller = zmq.Poller()
            poller.register(self._pipe[1], flags=zmq.POLLIN)
            poller.register(self.socket, flags=zmq.POLLIN)

            getLogger(__name__).debug(f'Listening for data for {self.cap_id}')
            self._pipe[1].send(b'')
            while True:
                avail = dict(poller.poll())
                if self._pipe[1] in avail:
                    getLogger(__name__).debug(f'Received shutdown order, terminating data acq. of {self}')
                    break
                data = self.receive()
                if not data:
                    getLogger(__name__).debug(f'Received null, capture data stream over')
                    break
                getLogger(__name__).debug(f'Received data snippet for {self}')
                self._accumulate_data(data)
            self._finish_accumulation()
            self._finalize_data()
            getLogger(__name__).info(f'Capture data for {self.cap_id} processed into {self.result.data.shape} '
                                     f'{self.result.data.dtype}: {self.result}')
        except zmq.ZMQError as e:
            getLogger(__name__).warning(f'Shutting down {self} due to {e}')
        finally:
            self.destablish()
            self._pipe[1].close()

    def data(self, timeout=None):
        self.join(timeout=timeout)
        return self.result

    def ready(self):
        self._pipe[0].recv()  # block until a byte arrives


class StreamCaptureSink(CaptureSink):
    def _finalize_data(self):
        # raw adc data is i0q0 i1q1 int16
        size = len(self._buf) / 2  # n int16
        n = int(size // np.prod(self._expected_buffer_shape[1:]))
        shape = (min(n, self._expected_buffer_shape[0]),) + self._expected_buffer_shape[1:]
        if n > self._expected_buffer_shape[0]:
            getLogger(__name__).warning(f'Received more data than expected for {self.cap_id}')
        elif n < self._expected_buffer_shape[0]:
            getLogger(__name__).warning(f'Finalizing incomplete capture data for {self.cap_id}')
        # TODO this might technically be a memory leak if we captured more data
        self.result = np.frombuffer(self._buf, count=np.prod(shape, dtype=int), dtype=np.int16).reshape(shape).squeeze()


class ADCCaptureSink(StreamCaptureSink):
    def _finalize_data(self):
        super()._finalize_data()
        self.result = ADCCaptureData(self.result)


class IQCaptureSink(StreamCaptureSink):
    def _finalize_data(self):
        super()._finalize_data()
        self.result = IQCaptureData(self.result)


class PhaseCaptureSink(StreamCaptureSink):
    def _finalize_data(self):
        super()._finalize_data()
        self.result = PhaseCaptureData(self.result)


class SimplePhotonSink(CaptureSink):
    def _accumulate_data(self, d):
        if self._buf is None:
            self._buf = []
        self._buf.append(blosc2.decompress(d))
        if sum(map(len, self._buf)) / 1024 ** 2 > 100:  # limit to 100MiB
            self._buf.pop(0)

    def _finish_accumulation(self):
        self._buf = b''.join(self._buf)

    def _finalize_data(self):
        self.result = np.frombuffer(self._buf, dtype=PHOTON_DTYPE)


class PhotonCaptureSink(CaptureSink):

    def capture(self, hdf, xymap, feedline_source, fl_ids):
        t = threading.Thread(target=self._main, args=(hdf, xymap, feedline_source, fl_ids))
        t.start()

    @staticmethod
    def _main(hdf, xymap, feedline_source, fl_ids, term_source='inproc://PhotonCaptureSink.terminator.inproc'):
        """

        Args:
            xymap: [nfeedline, npixel, 2] array
            feedline_source: zmq.PUB socket with photonbufers published by feedline
            term_source: a zmq socket of undecided type for detecting shutdown requests

        Returns: None

        """

        fl_npix = 2048
        n_fl = 5
        MAX_NEW_PHOTONS = 5000
        DETECTOR_SHAPE = (128, 80)
        fl_id_to_index = np.arange(n_fl, dtype=int)

        context = zmq.Context.instance()
        term = context.socket(zmq.SUB)
        term.setsockopt(zmq.SUBSCRIBE, id)
        term.connect(term_source)

        data = context.socket(zmq.SUB)
        data.setsockopt(zmq.SUBSCRIBE, fl_ids)
        data.connect(feedline_source)

        poller = zmq.Poller()
        poller.register(term, flags=zmq.POLLIN)
        poller.register(data, flags=zmq.POLLIN)

        live_image = np.zeros(DETECTOR_SHAPE)
        live_image_socket = None
        live_image_by_fl = live_image.reshape(n_fl, fl_npix)
        photons_rabuf = np.recarray(MAX_NEW_PHOTONS,
                                    dtype=(('time', 'u32'), ('x', 'u32'), ('y', 'u32'),
                                           ('phase', 'u16')))

        while True:
            avail = poller.poll()
            if term in avail:
                break

            frame = data.recv_multipart(copy=False)
            fl_id = frame[0]
            time_offset = frame[1]
            d = blosc2.decompress(frame[1])
            frame_duration = None  # todo time coverage of data
            # buffer is nchan*nmax+1 32bit: 16bit time(base2) 16bit phase
            # make array of to [nnmax+1, nchan, 2] uint16
            # nmax will always be <<2^12 number valid will be at [0,:,0]
            # times need oring w offset
            # photon data is d[1:d[0,i,0], i, :]

            nnew = d[0, :, 0].sum()
            # if we wanted to save binary data then we could save this, the x,y list, and the time offset
            # mean pixel count rate in this packet is simply [0,:,0]/dt
            fl_ndx = fl_id_to_index[fl_id]
            live_image_by_fl[fl_ndx, :] += d[0, :, 0] / frame_duration

            # if live_image_ready
            live_image_socket.send_multipart([f'liveim', blosc2.compress(live_image)])

            cphot = np.cumsum(d[0, :, 0], dtype=int)
            for i in range(fl_npix):
                sl_out = slice(cphot[i], cphot[i] + d[0, i, 0])
                sl_in = slice(1, d[0, i, 0])
                photons_rabuf['time'][sl_out] = d[sl_in, :, 0]
                photons_rabuf['time'][sl_out] |= time_offset
                photons_rabuf['phase'][sl_out] = d[sl_in, :, 1]
                photons_rabuf['x'][sl_out] = xymap[fl_ndx, i, 0]
                photons_rabuf['y'][sl_out] = xymap[fl_ndx, i, 1]
            hdf.grow_by(photons_rabuf[:nnew])

        term.close()
        data.close()
        hdf.close()


class PostageCaptureSink(CaptureSink):
    def _finalize_data(self):
        # get_postage returns:
        #  ids (nevents uint16 array or res cahnnels),
        #  events (nevents x 127 x2 of iq da
        ids, iqs = pickle.loads(self._buf)
        self.result = ids, IQCaptureData(iqs)


class StatusListener(threading.Thread):
    def __init__(self, id, source, initial_state='Created', start=True):
        super().__init__(name=f'StautsListner_{id}')
        self.daemon = True
        self.source = source
        self._pipe = zpipe(zmq.Context.instance())
        self.id = id
        self._status_messages = [initial_state]
        if start:
            self.start()

    def kill(self):
        self._pipe[0].send(b'')
        self._pipe[0].close()

    @staticmethod
    def is_final_status_message(update):
        """return True iff message is a final status update"""
        for r in CaptureRequest.FINAL_STATUSES:
            if update.startswith(r):
                return True
        return False

    def run(self):
        try:
            ctx = zmq.Context().instance()
            with ctx.socket(zmq.SUB) as sock:
                sock.linger = 0
                sock.setsockopt(zmq.SUBSCRIBE, self.id)
                sock.connect(self.source)

                poller = zmq.Poller()
                poller.register(self._pipe[1], flags=zmq.POLLIN)
                poller.register(sock, flags=zmq.POLLIN)
                getLogger(__name__).debug(f'Listening for status updates to {self.id}')
                self._pipe[1].send(b'')
                while True:
                    avail = dict(poller.poll())
                    if self._pipe[1] in avail:
                        getLogger(__name__).debug(f'Shutdown requested, terminating {self}')
                        break
                    elif sock not in avail:
                        time.sleep(.1)  # play nice
                        continue
                    id, update = sock.recv_multipart()
                    assert id == self.id or not self.id
                    update = update.decode()
                    self._status_messages.append(update)
                    getLogger(__name__).debug(f'Status update for {id}: {update}')
                    if self.is_final_status_message(update):
                        break
        except zmq.ZMQError as e:
            getLogger(__name__).critical(f"{self} died due to {e}")
        finally:
            self._pipe[1].close()

    def latest(self):
        return self._status_messages[-1]

    def history(self):
        return tuple(self._status_messages)

    def ready(self):
        self._pipe[0].recv()  # TODO make this line block


class ADCCaptureData:
    """
    Formats raw ADC capture data to complex floats representing mV at the ADC input SMA.
    """

    def __init__(self, raw_data):
        """
        ADC data captured by an ADC capture request
        Args:
            raw_data: raw data captured by adc capture
        """
        self.raw = raw_data

    @cached_property
    def data(self):
        return convert_adc_raw_to_mv(self.raw[:, 0] + 1j * self.raw[:, 1])


class IQCaptureData:
    """
    Formats raw IQ capture data to complex floats between +/- 1.
    """

    def __init__(self, raw_data):
        self.raw = raw_data

    @cached_property
    def data(self):
        return raw_iq_to_unit(self.raw[..., 0] + self.raw[..., 1] * 1j)


class PhaseCaptureData:
    def __init__(self, data):
        self.raw = data

    @cached_property
    def data(self):
        return raw_phase_to_radian(self.raw, scaled=False)


class PostageCaptureData:
    def __init__(self, raw_data):
        self.raw = raw_data

    @cached_property
    def data(self):
        return raw_iq_to_unit(self.raw)


class CaptureJob:
    def __init__(self, request: CaptureRequest, submit=False):
        """
        Args:
            request:
            submit: tuple of bools (with sink?, with status?) or True or False
        """
        self.request = request
        self.submitted = False

        self._status_listener = StatusListener(request.id, request.server.status_url,
                                               initial_state='created', start=False)

        if request.tap == 'adc':
            datasink = ADCCaptureSink
        elif request.tap == 'iq':
            datasink = IQCaptureSink
        elif request.tap == 'phase':
            datasink = PhaseCaptureSink
        elif request.tap == 'photon':
            datasink = SimplePhotonSink
        elif request.tap == 'postage':
            datasink = PostageCaptureSink
        else:
            raise ValueError(f'Unknown tap location {request.tap}')

        self.datasink = datasink(request, start=False)

        if submit:
            self.submit()
        elif isinstance(submit, tuple):
            self.submit(**submit)

    def __del__(self):
        self._kill_workers(kill_status_monitor=True, kill_data_sink=True)

    def status(self):
        """ Return the last known status of the request """
        return self._status_listener.latest()

    def status_history(self) -> tuple[str]:
        """Return a tuple of the status history of the job"""
        return self._status_listener.history()

    def cancel(self, kill_status_monitor=False, kill_data_sink=True):
        """

        Args:
            kill_status_monitor: Whether to terminate the status monitor
            kill_data_sink: Whether to terminate the data saver
        Returns: The parsed json submission result

        """
        self._kill_workers(kill_status_monitor=kill_status_monitor, kill_data_sink=kill_data_sink)
        ctx = zmq.Context().instance()
        with ctx.socket(zmq.REQ) as s:
            s.connect(self.request.server.command_url)
            s.send_pyobj(('abort', self.request.id))
            return s.recv_pyobj()

    def _kill_workers(self, kill_status_monitor=True, kill_data_sink=True):
        if kill_status_monitor and self._status_listener.is_alive():
            try:
                self.datasink.kill()
            except zmq.ZMQError as e:
                getLogger(__name__).warning(f'Caught {e} when telling data sink to terminate')
            if self._status_listener.is_alive():
                getLogger(__name__).warning(f'Status listener did not instantly terminate')
        if kill_data_sink and self.datasink.is_alive():
            try:
                self.datasink.kill()
            except zmq.ZMQError as e:
                getLogger(__name__).warning(f'Caught {e} when telling data sink to terminate')
            if self.datasink.is_alive():
                getLogger(__name__).warning(f'Data sink did not instantly terminate')

    def data(self, timeout=None):
        return self.datasink.data(timeout=timeout)

    def submit(self, with_sink=False, with_status=True):
        if with_status:
            self._status_listener.start()
            self._status_listener.ready()

        if with_sink and not self.request.data_endpoint.startswith('file://'):
            self.datasink.start()
            self.datasink.ready()

        try:
            ret = self.request.submit()
            self.submitted = True
            return ret
        except Exception as e:
            self._status_listener.kill()
            self.datasink.kill()
            raise e


class PowerSweepJob:
    def __init__(self, ntones=2048, points=512, min_attn=0, max_attn=30, attn_step=0.25, lo_center=0, fres=7.14e3,
                 use_cached=True):
        """
        Args:
            ntones (int): Number of tones in power sweep comb. Default is 2048.
            points (int): Number of I and Q samples to capture for each IF setting.
            min_attn (float): Lowest global attenuation value in dB. 0-30 dB allowed.
            max_attn (float): Highest global attenuation value in dB. 0-30 dB allowed.
            attn_step (float): Difference in dB between subsequent global attenuation settings.
                               0.25 dB is default and finest resolution.
            lo_center (float): Starting LO position in Hz. Default is XXX XX-XX allowed.
            fres (float): Difference in Hz between subsequent LO settings.
                               7.14e3 Hz is default and finest resolution we can produce with a 4.096 GSPS DAC
                               and 2**19 complex samples in the waveform look-up-table.

        Returns:
            PowerSweepRequest: Object which computes the appropriate hardware settings and produces the necessary
            CaptureRequests to collect power sweep data.
        """
        self.freqs = np.linspace(0, ntones - 1, ntones)
        self.points = points
        self.total_attens = np.arange(min_attn, max_attn + attn_step, attn_step)
        self._sweep_bw = SYSTEM_BANDWIDTH / ntones
        self.lo_centers = compute_lo_steps(center=lo_center, resolution=fres, bandwidth=self._sweep_bw)
        self.use_cached = use_cached

    def generate_jobs(self, submit=False) -> List[CaptureJob]:
        from .feedline_config import WaveformConfig, IFConfig

        feedline_server = 'tcp://localhost:8888'
        capture_data_server = 'tcp://localhost:8889'
        status_server = 'tcp://localhost:8890'

        dacconfig = WaveformConfig(n_uniform_tones=1024)
        dacconfig_hash = hash(dacconfig)
        jobs = []
        for adc_atten, dac_atten in self.attens:
            for freq in self.lo_centers:
                ifconfig = IFConfig(lo=freq, adc_attn=adc_atten, dac_attn=dac_atten)
                fc = FeedlineConfig(dac_setup=dacconfig_hash if jobs else dacconfig, if_setup=ifconfig)
                cr = CaptureRequest(self.points, 'adc', feedline_config=fc)
                cj = CaptureJob(cr, feedline_server, capture_data_server, status_server, submit=False)
                jobs.append(cj)

        if submit:
            try:
                for j in jobs:
                    j.submit()
            except Exception as e:
                getLogger(__name__).debug('Cancelling all capture jobs use to a submission error.')
                for j in jobs:
                    j.cancel(kill_status_monitor=True, kill_data_sink=True)
                raise e

        return jobs


class PowerSweepRequest:
    def __init__(self, ntones=2048, points=512, min_attn=0, max_attn=30, attn_step=0.25, lo_center=0, fres=7.14e3,
                 use_cached=True):
        """
        Args:
            ntones (int): Number of tones in power sweep comb. Default is 2048.
            points (int): Number of I and Q samples to capture for each IF setting.
            min_attn (float): Lowest global attenuation value in dB. 0-30 dB allowed.
            max_attn (float): Highest global attenuation value in dB. 0-30 dB allowed.
            attn_step (float): Difference in dB between subsequent global attenuation settings.
                               0.25 dB is default and finest resolution.
            lo_center (float): Starting LO position in Hz. Default is XXX XX-XX allowed.
            fres (float): Difference in Hz between subsequent LO settings.
                               7.14e3 Hz is default and finest resolution we can produce with a 4.096 GSPS DAC
                               and 2**19 complex samples in the waveform look-up-table.

        Returns:
            PowerSweepRequest: Object which computes the appropriate hardware settings and produces the necessary
            CaptureRequests to collect power sweep data.

        """
        self.freqs = np.linspace(0, ntones - 1, ntones)
        self.points = points
        self.total_attens = np.arange(min_attn, max_attn + attn_step, attn_step)
        self._sweep_bw = SYSTEM_BANDWIDTH / ntones
        self.lo_centers = compute_lo_steps(center=lo_center, resolution=fres, bandwidth=self._sweep_bw)
        self.use_cached = use_cached

    def capture_requests(self):
        dacsetup = WaveformConfig('power_sweep_comb', n_uniform_tones=self.ntones)
        return [CaptureRequest(self.samples, FeedlineConfig(dac_setup=dacsetup,
                                                            if_config=IFConfig(lo=freq, adc_attn=adc_atten,
                                                                               dac_attn=dac_atten)))
                for (adc_atten, dac_atten) in self.attens for freq in self.lo_centers]
